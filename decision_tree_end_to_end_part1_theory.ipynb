{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.Can be used for classification as well as regression, but mostly used for classification__\n",
    "\n",
    "__2.Can be used for binary (eg. yes/no, true/false) and also for multi class classficiation(eg. grouping of the alphabets)__\n",
    "\n",
    "__3.Decision Tree algo finds the relation between the independent and dependent variables and represent them in form of a tree structure__\n",
    "\n",
    "__4. The tree structure can also be visualized in the form of rules__\n",
    "\n",
    "__5. It's a non parametric algorithm, meaning there are no assumptions made on the underlying data distribution__\n",
    "\n",
    "__6. The first node is called root node, the intermediate nodes are called branches and the final nodes are called leaf nodes__\n",
    "\n",
    "__7. The errors can happen at training or testing stage and are called training and testing errors respectively__\n",
    "\n",
    "__8. When the tree becomes too large, they become overfit meaning, they will perform good on the training set where as they will fail on testing/production data__\n",
    "\n",
    "__9.Posterior Probability: we control the algorithm restricting it from over growing and this process is called regularization which may result in the tree having the leafs which are not homogenous.In such situation we use the probability to classify the test record__\n",
    "\n",
    "__10.Some of the feature is selected from the root node and they are further broken to branches based on some function applied on them.how does the algorithm choose the column on which the function is applied? it's done through concept called Impurity and Loss function__\n",
    "\n",
    "__11.The Decision tree algo learns and creates the tree by optimization of the loss function.__\n",
    "\n",
    "__12. To understand the loss function we need to understand how loss function calculates the impurity /purity of the target column.__\n",
    "\n",
    "__13. The measure of impurity is Entropy & Gini__\n",
    "\n",
    "__14. More heterogenous more uncertain, more homogenous more certain. Entropy/Gini are measures of entropy__\n",
    "\n",
    "__15. This Decision Tree is also called CART (Classification & Regression Tree) algorithm.There are other algorithms, scikit uses CART__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shanon's Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg: if a bag has 6 orange,4 white balls then the entropy H(x) of the bag is calculated as below\n",
    "\n",
    "N = number of classe\n",
    "\n",
    "ß = sigma(from 0 to N-1, in our case N-1 = 2-1, so consider ß as sigma i= 0 to  1)\n",
    "\n",
    "in the following log is log base 2\n",
    "\n",
    "H(x) = -ß P(i)*log(P(i))\n",
    "\n",
    "\n",
    "= -P(Orange)*log(P(Orange))-P(white)*log(P(white))\n",
    "\n",
    "= -(6/10)*log(6/10)-P(4/10)*log(4/10)\n",
    "\n",
    "= -(0.6*log(0.6)) -(0.4*log(0.4)\n",
    "\n",
    "=- (-0.44214-0.5287)\n",
    "\n",
    "H(x) = 0.9709\n",
    "\n",
    "\n",
    "in case we remove all the white balls then the entropy will be\n",
    "\n",
    "H(x) = -[1 * log(1)  + 0 * log(0)]\n",
    "\n",
    "H(x) = 0\n",
    "\n",
    "if entropy is 0, then it means the information is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "\n",
    "The decision tree algorithm breaks the data set in to root node,branches and leaf nodes in such a way, that the entropy gets reduced as we travel from root node to the downwards. say if the entropy at root node is E1 and the entropy at the next immediate branch node is E2, then \n",
    "\n",
    "__information gain = E2-E1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini\n",
    "\n",
    "Gini index is also a measure of purity/Impurity\n",
    "\n",
    "ß = sigma(from i = 0 to N-1, where N = no of classe)\n",
    "\n",
    "gini = 1- ß(P(i)^2)\n",
    "\n",
    "CART algo uses the gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain using Entropy\n",
    "\n",
    "<img src=\"entropy_calculation.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information gain using Gini index\n",
    "\n",
    "<img src=\"gini.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros\n",
    "\n",
    "\n",
    "__quite fast in training & testing__\n",
    "\n",
    "__can handle numerical and categorical values__\n",
    "\n",
    "__simple to understand by visualizing the rules__\n",
    "\n",
    "\n",
    "\n",
    "# Cons\n",
    "\n",
    "__Greedy algo : often biased towards the feature with large cardinal values__\n",
    "\n",
    "__Tend to become overfit creating large trees__\n",
    "\n",
    "__Large trees are difficult to understand__\n",
    "\n",
    "__Small change in training set may result in large changes to the tree__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to avoid the overfitting of the Decision tree\n",
    "\n",
    "The overfitting problem or complex growth of the tree of the decision tree can be avoided by regularization. Regularization is controlling the CART algorithm. Following are the some parameters which can be used for regularization. Overfitting results in the modeling of the noise in the data\n",
    "\n",
    "__1. max_depth__  : maximum length from root node to the leaf node\n",
    "\n",
    "__2. min_sample_split__ : min_samples_split specifies the minimum number of samples required to split an internal node\n",
    "\n",
    "__3. min_sample_leaf__ : min_samples_leaf specifies the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "For instance, if min_samples_split = 5, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If min_samples_leaf = 2, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less then the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "__4. min_weight_fraction_leaf__: same as min_sample_leaf but expressed in fraction of total weighted instances\n",
    "\n",
    "__5. max_leaf_nodes__ : maximum number of leaf nodes in a tree\n",
    "\n",
    "__6. max_feature_size__ : maximum number of features to be evaluvated for splitting each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
